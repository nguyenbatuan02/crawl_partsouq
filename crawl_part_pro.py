import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import json
import time
import os
from multiprocessing import Process, Queue
import queue

class PartsouqHTMLSaver:
    def __init__(self):
        # Setup undetected Chrome 
        options = uc.ChromeOptions()
        # options.add_argument('--headless=new')  
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--window-size=1920,1080')
        
        self.driver = uc.Chrome(options=options, version_main=None)
        self.base_url = "https://partsouq.com"
        
        # T·∫°o th∆∞ m·ª•c l∆∞u HTML
        self.html_folder = 'html_sources'
        os.makedirs(self.html_folder, exist_ok=True)
        
        # Th∆∞ m·ª•c backup
        self.backup_folder = 'backups'
        os.makedirs(self.backup_folder, exist_ok=True)
        
        # Tracking folder ƒë√£ d√πng ƒë·ªÉ tr√°nh tr√πng
        self.used_folders = {}
        self.current_model_folder = None
    
    def load_json(self, filename):
        """Load data from JSON file"""
        with open(filename, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def save_json(self, data, filename):
        """Save data to JSON file"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"‚úÖ ƒê√£ l∆∞u file: {filename}")
    
    def save_backup(self, data, brand_name, car_type_idx, model_idx, worker_id=""):
        """L∆∞u backup sau m·ªói model"""
        safe_brand = self._safe_filename(brand_name)
        
        # Th√™m worker_id ƒë·ªÉ tr√°nh conflict
        if worker_id:
            backup_filename = f"{safe_brand}_CarType{car_type_idx}_Model{model_idx}_W{worker_id}.json"
        else:
            backup_filename = f"{safe_brand}_CarType{car_type_idx}_Model{model_idx}.json"
        
        backup_path = os.path.join(self.backup_folder, backup_filename)
        
        self.save_json(data, backup_path)
        print(f"üíæ BACKUP: {backup_path}")
    
    def set_current_model_folder(self, brand, car_type, model):
        """Set folder cho model hi·ªán t·∫°i"""
        base_model_folder = os.path.join(
            self.html_folder,
            self._safe_filename(brand),
            self._safe_filename(car_type),
            self._safe_filename(model)
        )
        
        self.current_model_folder = self._get_unique_folder(base_model_folder)
        print(f"  üìÅ Model folder: {self.current_model_folder}")
    
    def _get_unique_folder(self, base_path):
        """T·∫°o t√™n th∆∞ m·ª•c unique n·∫øu b·ªã tr√πng"""
        if base_path not in self.used_folders:
            if not os.path.exists(base_path):
                self.used_folders[base_path] = base_path
                return base_path
            else:
                counter = 1
                while True:
                    new_path = f"{base_path}{counter}"
                    if not os.path.exists(new_path):
                        self.used_folders[base_path] = new_path
                        return new_path
                    counter += 1
        
        return self.used_folders[base_path]
    
    def _safe_filename(self, name):
        """Chuy·ªÉn t√™n th√†nh t√™n file an to√†n"""
        safe = name.replace('/', '').replace('\\', '').replace(':', '_')
        safe = safe.replace('*', '').replace('?', '').replace('"', '_')
        safe = safe.replace('<', '').replace('>', '').replace('|', '_')
        safe = safe.replace(' ', '_').strip()
        
        if len(safe) > 100:
            safe = safe[:100]
        
        return safe
    
    def save_html(self, url, brand, car_type, model, category, title):
        """Truy c·∫≠p URL, l∆∞u HTML V√Ä crawl parts"""
        try:
            print(f"      üåê ƒêang truy c·∫≠p: {url}")
            self.driver.get(url)
            
            # Ch·ªù Cloudflare
            print(f"      ‚è≥ Ch·ªù Cloudflare...")
            time.sleep(3)
            
            # Wait for page load
            print(f"      ‚è≥ Ch·ªù load trang...")
            wait = WebDriverWait(self.driver, 15)
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ".table-bordered-1")))
            
            # L·∫•y HTML source
            html_content = self.driver.page_source
            
            if not self.current_model_folder:
                raise Exception("Ch∆∞a set current_model_folder!")
            
            # T·∫°o ƒë∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß
            folder_path = os.path.join(
                self.current_model_folder,
                self._safe_filename(category)
            )
            os.makedirs(folder_path, exist_ok=True)
            
            filename = self._safe_filename(title) + '.html'
            filepath = os.path.join(folder_path, filename)
            
            # L∆∞u HTML
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            relative_path = os.path.relpath(filepath, '.')
            print(f"      üìÑ HTML: {relative_path}")
            
            # CRAWL PARTS DATA
            print(f"      üîß ƒêang parse parts...")
            parts = self._parse_parts()
            print(f"      ‚úÖ Parts: {len(parts)} items")
            
            return relative_path, parts
            
        except Exception as e:
            print(f"      ‚ùå L·ªói save_html: {e}")
            return None, []
    
    def _parse_parts(self):
        """Parse parts"""
        try:
            parts = []
            
            tables = self.driver.find_elements(By.CSS_SELECTOR, ".table-bordered-1")
            if not tables:
                print("     ‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y table parts")
                return []
            
            table = tables[0]
            
            # ƒê·ªåC HEADERS
            try:
                headers = table.find_elements(By.CSS_SELECTOR, "thead tr th")
                header_names = []
                
                for h in headers:
                    header_text = h.text.strip()
                    if header_text:
                        safe_name = header_text.lower().replace(' ', '').replace('-', '')
                        safe_name = safe_name.replace('/', '_').replace('(', '').replace(')', '')
                        header_names.append(safe_name)
                    else:
                        header_names.append(f"col_{len(header_names)}")
                
                print(f"     üìã Headers ({len(header_names)} c·ªôt): {header_names}")
                
                if not header_names:
                    print("     ‚ùå Kh√¥ng c√≥ headers!")
                    return []
                
            except Exception as e:
                print(f"     ‚ùå L·ªói ƒë·ªçc headers: {e}")
                return []
            
            # PARSE ROWS
            rows = table.find_elements(By.CSS_SELECTOR, "tbody tr.part-search-tr")
            
            if not rows:
                all_rows = table.find_elements(By.CSS_SELECTOR, "tbody tr")
                rows = [row for row in all_rows if not row.find_elements(By.TAG_NAME, "th")]
            
            print(f"     üîç T√¨m th·∫•y {len(rows)} rows")
            
            for row_idx, row in enumerate(rows, 1):
                try:
                    cells = row.find_elements(By.TAG_NAME, "td")
                    
                    if len(cells) < 1:
                        continue
                    
                    part_data = {}
                    
                    for col_idx, cell in enumerate(cells):
                        if col_idx < len(header_names):
                            field_name = header_names[col_idx]
                        else:
                            field_name = f"col_{col_idx}"
                        
                        links = cell.find_elements(By.TAG_NAME, "a")
                        if links:
                            value = links[0].text.strip()
                        else:
                            value = cell.text.strip()
                        
                        if value:
                            part_data[field_name] = value
                    
                    if part_data:
                        parts.append(part_data)
                        
                        if row_idx == 1:
                            print(f"     ‚úÖ Sample: {part_data}")
                    
                except Exception as e:
                    if row_idx <= 2:
                        print(f"     ‚ö†Ô∏è  L·ªói row {row_idx}: {e}")
                    continue
            
            print(f"     ‚úÖ Crawled {len(parts)} parts")
            return parts
            
        except Exception as e:
            print(f"     ‚ùå L·ªói parse parts: {e}")
            return []
    
    def close(self):
        """Close browser"""
        self.driver.quit()


# ‚≠ê WORKER FUNCTION - M·ªói process ch·∫°y function n√†y
def worker_crawl_model(worker_id, model_data, brand_name, car_type_name, car_type_idx, model_idx, output_queue):
    """
    Worker function ƒë·ªÉ crawl 1 model
    M·ªói worker ch·∫°y trong process ri√™ng v·ªõi browser ri√™ng
    """
    print(f"\nüîµ [Worker {worker_id}] B·∫Øt ƒë·∫ßu crawl Model {model_idx}: {model_data['name']}")
    
    try:
        # T·∫°o instance crawler ri√™ng cho worker n√†y
        saver = PartsouqHTMLSaver()
        
        model_name = model_data['name']
        
        # SET FOLDER CHO MODEL
        saver.set_current_model_folder(brand_name, car_type_name, model_name)
        
        model_start_time = time.time()
        
        # Crawl t·∫•t c·∫£ categories trong model n√†y
        for category in model_data.get('categories', []):
            category_name = category['category']
            print(f"\n    [W{worker_id}] üìÅ Category: {category_name}")
            print(f"    [W{worker_id}] üìã Titles: {len(category.get('titles', []))}")
            
            # Loop through titles
            for idx, title in enumerate(category.get('titles', []), 1):
                title_name = title['title']
                title_url = title['url']
                
                print(f"\n      [W{worker_id}] [{idx}/{len(category['titles'])}] üìù {title_name}")
                
                # L∆∞u HTML V√Ä crawl parts
                html_file, parts = saver.save_html(
                    title_url,
                    brand_name,
                    car_type_name,
                    model_name,
                    category_name,
                    title_name
                )
                
                if html_file:
                    title['html_file'] = html_file
                    title['parts'] = parts
                else:
                    title['html_file'] = None
                    title['parts'] = []
        
        model_elapsed = time.time() - model_start_time
        print(f"\n  [W{worker_id}] ‚è±Ô∏è  Ho√†n th√†nh Model {model_name} trong {model_elapsed/60:.1f} ph√∫t")
        
        # ƒê√≥ng browser
        saver.close()
        
        # Tr·∫£ k·∫øt qu·∫£ v·ªÅ main process qua queue
        output_queue.put({
            'worker_id': worker_id,
            'car_type_idx': car_type_idx,
            'model_idx': model_idx,
            'model_data': model_data,
            'success': True
        })
        
    except Exception as e:
        print(f"\n‚ùå [Worker {worker_id}] L·ªói: {e}")
        import traceback
        traceback.print_exc()
        
        output_queue.put({
            'worker_id': worker_id,
            'car_type_idx': car_type_idx,
            'model_idx': model_idx,
            'model_data': None,
            'success': False,
            'error': str(e)
        })


# ‚≠ê MAIN FUNCTION - Qu·∫£n l√Ω parallel crawling
def parallel_crawl(input_file, output_file, num_workers=2):
    """
    Crawl song song v·ªõi s·ªë l∆∞·ª£ng workers t√πy ch·ªânh
    num_workers: S·ªë l∆∞·ª£ng browser ch·∫°y ƒë·ªìng th·ªùi (2-3 khuy·∫øn ngh·ªã)
    """
    
    # Load data
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # Copy sang output file
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"‚úÖ ƒê√£ copy {input_file} ‚Üí {output_file}")
    
    # T·∫°o danh s√°ch t·∫•t c·∫£ models c·∫ßn crawl
    model_queue = []
    
    for brand in data:
        brand_name = brand['brand']
        
        for car_type_idx, car_type in enumerate(brand.get('car_types', []), 1):
            car_type_name = car_type['car_type']
            
            for model_idx, model in enumerate(car_type.get('models', []), 1):
                model_queue.append({
                    'brand_name': brand_name,
                    'car_type_name': car_type_name,
                    'car_type_idx': car_type_idx,
                    'model_idx': model_idx,
                    'model_data': model
                })
    
    print(f"\nüìä T·ªïng s·ªë models c·∫ßn crawl: {len(model_queue)}")
    print(f"üî¢ S·ªë workers: {num_workers}")
    print(f"‚è±Ô∏è  ∆Ø·ªõc t√≠nh th·ªùi gian: {len(model_queue) / num_workers:.1f}x nhanh h∆°n\n")
    
    # Queue ƒë·ªÉ nh·∫≠n k·∫øt qu·∫£ t·ª´ workers
    output_queue = Queue()
    
    # Tracking
    active_processes = []
    completed = 0
    failed = 0
    
    # Crawl t·ª´ng batch
    model_idx_in_queue = 0
    
    while model_idx_in_queue < len(model_queue) or active_processes:
        
        # Start workers cho batch m·ªõi (n·∫øu c√≤n slot tr·ªëng)
        while len(active_processes) < num_workers and model_idx_in_queue < len(model_queue):
            
            task = model_queue[model_idx_in_queue]
            worker_id = model_idx_in_queue + 1
            
            print(f"\nüöÄ Kh·ªüi ƒë·ªông Worker {worker_id} cho Model: {task['model_data']['name']}")
            
            # T·∫°o process m·ªõi
            p = Process(
                target=worker_crawl_model,
                args=(
                    worker_id,
                    task['model_data'],
                    task['brand_name'],
                    task['car_type_name'],
                    task['car_type_idx'],
                    task['model_idx'],
                    output_queue
                )
            )
            
            p.start()
            active_processes.append({
                'process': p,
                'worker_id': worker_id,
                'task': task
            })
            
            model_idx_in_queue += 1
        
        # Check xem c√≥ worker n√†o ho√†n th√†nh ch∆∞a
        time.sleep(2)  # ƒê·ª£i 2s tr∆∞·ªõc khi check
        
        # Thu th·∫≠p k·∫øt qu·∫£ t·ª´ queue (non-blocking)
        try:
            while True:
                result = output_queue.get_nowait()
                
                if result['success']:
                    print(f"\n‚úÖ Worker {result['worker_id']} ho√†n th√†nh!")
                    
                    # C·∫≠p nh·∫≠t data v·ªõi k·∫øt qu·∫£ m·ªõi
                    for brand in data:
                        for car_type in brand.get('car_types', []):
                            for model in car_type.get('models', []):
                                if model['name'] == result['model_data']['name']:
                                    model.update(result['model_data'])
                                    break
                    
                    # L∆∞u backup
                    with open(output_file, 'w', encoding='utf-8') as f:
                        json.dump(data, f, ensure_ascii=False, indent=2)
                    
                    completed += 1
                else:
                    print(f"\n‚ùå Worker {result['worker_id']} th·∫•t b·∫°i: {result.get('error', 'Unknown')}")
                    failed += 1
                
                print(f"\nüìä Ti·∫øn ƒë·ªô: {completed}/{len(model_queue)} ho√†n th√†nh, {failed} th·∫•t b·∫°i, {len(active_processes)} ƒëang ch·∫°y")
                
        except queue.Empty:
            pass
        
        # Lo·∫°i b·ªè processes ƒë√£ k·∫øt th√∫c
        active_processes = [p for p in active_processes if p['process'].is_alive()]
    
    # Ch·ªù t·∫•t c·∫£ processes k·∫øt th√∫c
    for p_info in active_processes:
        p_info['process'].join()
    
    print(f"\n{'='*60}")
    print(f"‚ú® HO√ÄN TH√ÄNH!")
    print(f"   ‚úÖ Th√†nh c√¥ng: {completed}")
    print(f"   ‚ùå Th·∫•t b·∫°i: {failed}")
    print(f"{'='*60}")


# Main execution
if __name__ == "__main__":
    
    # ‚öôÔ∏è C·∫§U H√åNH
    INPUT_FILE = "Nissan_Progress_CT8.json"  # File input
    OUTPUT_FILE = "Nissan.json"   # File output
    NUM_WORKERS = 3  #  S·ªë browser ch·∫°y ƒë·ªìng th·ªùi 
    
    print(f"üì• Input: {INPUT_FILE}")
    print(f"üì§ Output: {OUTPUT_FILE}")
    print(f"üî¢ Workers: {NUM_WORKERS}")
    print("="*60)
    
    parallel_crawl(INPUT_FILE, OUTPUT_FILE, NUM_WORKERS)