import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import json
import time
import os

class PartsouqCrawler:
    def __init__(self):
        # Setup undetected Chrome ƒë·ªÉ bypass Cloudflare
        options = uc.ChromeOptions()
        # options.add_argument('--headless=new')  # Uncomment ƒë·ªÉ ch·∫°y ng·∫ßm
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--window-size=1920,1080')
        
        self.driver = uc.Chrome(options=options, version_main=None)
        self.base_url = "https://partsouq.com"
    
    def load_json(self, filename):
        """Load data from JSON file"""
        with open(filename, 'r', encoding='utf-8') as f:
            return json.load(f)
        
    def get_all_brands(self):
        """Crawl all brand links from homepage"""
        
        try:
            self.driver.get(self.base_url)
            
            # Ch·ªù Cloudflare check
            time.sleep(8)
            
            # Wait for brand container
            wait = WebDriverWait(self.driver, 20)
            wait.until(EC.presence_of_element_located((By.ID, "make-icons")))
            
            # Find all brand links
            brand_elements = self.driver.find_elements(By.CSS_SELECTOR, "#make-icons .item a")
            
            brands = []
            for element in brand_elements:
                try:
                    brand_name = element.find_element(By.CLASS_NAME, "shop-title").text
                    brand_href = element.get_attribute("href")
                    
                    brands.append({
                        "brand": brand_name,
                        "href": brand_href
                    })
                    
                    print(f"Found: {brand_name} - {brand_href}")
                    
                except Exception as e:
                    print(f"L·ªói khi parse brand: {e}")
                    continue
            
            return brands
            
        except Exception as e:
            print(f"L·ªói khi crawl brands: {e}")
            return []
    
    def get_car_types(self, brand_url):
        """Get all car types/models for a brand"""
        print(f"\n ƒêang crawl car types t·ª´: {brand_url}")
        
        try:
            self.driver.get(brand_url)
            
            # Ch·ªù Cloudflare check
            time.sleep(6)
            
            # Wait for panel to load
            wait = WebDriverWait(self.driver, 20)
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ".panel-heading")))
            
            car_types = []
            seen_urls = set()
            
            # T√¨m t·∫•t c·∫£ links c√≥ href ch·ª©a '/catalog/genuine/pick'
            all_links = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='/catalog/genuine/pick']")
            
            print(f" T√¨m th·∫•y {len(all_links)} links...")
            
            for link in all_links:
                try:
                    car_type = link.text.strip()
                    car_href = link.get_attribute("href")
                    
                    if car_type and car_href and car_href not in seen_urls:
                        car_type = car_type.replace('\n', ' ').strip()
                        
                        car_types.append({
                            "car_type": car_type,
                            "href": car_href
                        })
                        
                        seen_urls.add(car_href)
                        print(f"   {car_type}")
                        
                except Exception as e:
                    continue
            
            return car_types
            
        except Exception as e:
            print(f"   L·ªói khi crawl car types: {e}")
            try:
                self.driver.save_screenshot("error_screenshot.png")
            except:
                pass
            return []
    
    def get_models(self, car_type_url):
        """Get all models for a car type"""
        print(f"\n     ƒêang crawl models t·ª´: {car_type_url}")
        
        try:
            self.driver.get(car_type_url)
            
            # Ch·ªù Cloudflare
            time.sleep(5)
            
            # Wait for table to load
            wait = WebDriverWait(self.driver, 20)
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ".search-result-vin")))
            
            models = []
            seen_urls = set()
            
            # T√¨m t·∫•t c·∫£ rows trong table (b·ªè qua header row)
            rows = self.driver.find_elements(By.CSS_SELECTOR, ".search-result-vin tbody tr:not(:first-child)")
            
            print(f"       T√¨m th·∫•y {len(rows)} models...")
            
            for row in rows:
                try:
                    cells = row.find_elements(By.TAG_NAME, "td")
                    if len(cells) >= 5:
                        name = cells[0].text.strip()
                        description = cells[1].text.strip()
                        model = cells[2].text.strip()
                        options = cells[3].text.strip()
                        prod_period = cells[4].text.strip()
                        
                        # L·∫•y URL t·ª´ Model column
                        model_link = cells[2].find_element(By.TAG_NAME, "a")
                        model_url = model_link.get_attribute("href")
                        
                        if model and model_url and model_url not in seen_urls:
                            models.append({
                                "name": name,
                                "description": description,
                                "model": model,
                                "options": options,
                                "prod_period": prod_period,
                                "url": model_url
                            })
                            
                            seen_urls.add(model_url)
                            print(f"       {model} - {description}")
                            
                except Exception as e:
                    print(f"       L·ªói parse row: {e}")
                    continue
            
            return models
            
        except Exception as e:
            print(f"       L·ªói khi crawl models: {e}")
            try:
                self.driver.save_screenshot("error_models.png")
                print("      ƒê√£ l∆∞u screenshot: error_models.png")
            except:
                pass
            return []
    
    def get_categories_and_titles(self, model_url):
        """Get all categories and their titles/diagrams"""
        print(f"\n        ƒêang crawl categories t·ª´: {model_url}")
        
        try:
            self.driver.get(model_url)
            
            # Ch·ªù Cloudflare
            print("         ƒêang ch·ªù Cloudflare...")
            time.sleep(5)
            
            # Wait for page to load
            wait = WebDriverWait(self.driver, 20)
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ".vehicle-tg")))
            
            categories = []
            
            # B∆Ø·ªöC 1: Crawl DEFAULT CATEGORY
            try:
                default_category_name = self.driver.find_element(By.CSS_SELECTOR, "h2.current-category").text.strip()
                default_category_name = default_category_name.replace(" Diagrams", "").strip()
                
                print(f"         Default Category: {default_category_name}")
                
                # Crawl titles ƒëang hi·ªÉn th·ªã
                default_titles = self.get_titles_only()
                
                categories.append({
                    "category": default_category_name,
                    "url": model_url,
                    "titles": default_titles
                })
                
                print(f"             {len(default_titles)} titles found")
                
            except Exception as e:
                print(f"         Kh√¥ng crawl ƒë∆∞·ª£c default category: {e}")
            
            # B∆Ø·ªöC 2: Thu th·∫≠p danh s√°ch CHILD CATEGORIES
            category_info_list = []
            seen_urls = set()
            
            category_rows = self.driver.find_elements(By.CSS_SELECTOR, ".vehicle-tg tbody tr")
            print(f"         T√¨m th·∫•y {len(category_rows)} rows trong sidebar")
            
            for idx, row in enumerate(category_rows):
                try:
                    # Ki·ªÉm tra xem row n√†y c√≥ link kh√¥ng
                    links = row.find_elements(By.TAG_NAME, "a")
                    
                    if links:
                        # Row c√≥ link - ƒë√¢y l√† category c√≥ th·ªÉ click
                        link = links[0]
                        category_name = link.text.strip()
                        category_url = link.get_attribute("href")
                        
                        if category_name and category_url and category_url not in seen_urls:
                            category_info_list.append({
                                "name": category_name,
                                "url": category_url
                            })
                            seen_urls.add(category_url)
                            print(f"         Found Category: {category_name}")
                    else:
                        # Row kh√¥ng c√≥ link - parent category
                        try:
                            cell_text = row.find_element(By.TAG_NAME, "td").text.strip()
                            if cell_text:
                                print(f"         Parent: {cell_text}")
                        except:
                            pass
                        
                except Exception as e:
                    print(f"         L·ªói parse row {idx}: {e}")
                    continue
            
            print(f"         T·ªïng s·ªë child categories: {len(category_info_list)}")
            
            # B∆Ø·ªöC 3: Crawl titles cho t·ª´ng child category
            for idx, cat_info in enumerate(category_info_list, 1):
                print(f"\n        [{idx}/{len(category_info_list)}]  Crawling: {cat_info['name']}")
                
                # Navigate to category
                self.driver.get(cat_info['url'])
                time.sleep(4)
                
                # Get titles (kh√¥ng l·∫•y parts)
                titles = self.get_titles_only()
                
                categories.append({
                    "category": cat_info['name'],
                    "url": cat_info['url'],
                    "titles": titles
                })
                
                print(f"             {len(titles)} titles found")
            
            return categories
        
        except Exception as e:
            print(f"       L·ªói khi crawl categories: {e}")
            try:
                self.driver.save_screenshot("error_categories.png")
            except:
                pass
            return []
        
    def get_titles_only(self):
        """Get all titles from current page WITHOUT crawling parts"""
        try:
            wait = WebDriverWait(self.driver, 15)
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ".thumbnail")))
            
            titles = []
            seen_urls = set()
            
            # Find all diagram thumbnails
            thumbnails = self.driver.find_elements(By.CSS_SELECTOR, ".thumbnail")
            
            print(f"             T√¨m th·∫•y {len(thumbnails)} thumbnails")
            
            for thumb in thumbnails:
                try:
                    # Get title from caption h5 > a
                    caption = thumb.find_element(By.CSS_SELECTOR, ".caption h5 a")
                    title_text = caption.text.strip()
                    title_url = caption.get_attribute("href")
                    
                    if title_text and title_url and title_url not in seen_urls:
                        titles.append({
                            "title": title_text,
                            "url": title_url
                        })
                        
                        seen_urls.add(title_url)
                        print(f"              {title_text}")
                        
                except Exception as e:
                    continue
            
            return titles
            
        except Exception as e:
            print(f"           L·ªói khi crawl titles: {e}")
            return []
    
    def save_to_json(self, data, filename):
        """Save data to JSON file"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"\n ƒê√£ l∆∞u v√†o file: {filename}")
    
    def close(self):
        """Close browser"""
        self.driver.quit()


# Main execution - RESUME FROM BACKUP
if __name__ == "__main__":
    crawler = PartsouqCrawler()
    
    # ‚öôÔ∏è C·∫§U H√åNH - ƒêI·ªÄN TH·ª¶ C√îNG
    TARGET_BRAND = "Toyota"
    RESUME_FROM_BACKUP = "Toyota_CT1_Model1.json"  #  ƒêi·ªÅn t√™n file backup g·∫ßn nh·∫•t
    
    #  Parse t√™n file ƒë·ªÉ l·∫•y v·ªã tr√≠
    # Format: Brand_CTx_Modely.json
    try:
        filename = RESUME_FROM_BACKUP.replace('.json', '')
        parts = filename.split('_')
        
        ct_part = [p for p in parts if p.startswith('CT')][0]
        model_part = [p for p in parts if p.startswith('Model')][0]
        
        START_CT = int(ct_part.replace('CT', ''))
        START_MODEL = int(model_part.replace('Model', ''))
        
        print(f"\nüìç Parse t·ª´ filename: CT={START_CT}, Model={START_MODEL}")
        print(f"‚ñ∂Ô∏è  S·∫Ω resume t·ª´ CT{START_CT} Model{START_MODEL + 1}")
        
    except Exception as e:
        print(f"\n‚ùå L·ªói parse filename: {e}")
        print(f"Format ƒë√∫ng: Brand_CTx_Modely.json")
        exit(1)
    
    try:
        
        # Load backup data
        if os.path.exists(RESUME_FROM_BACKUP):
            print(f"\n‚úÖ ƒê√£ t√¨m th·∫•y file backup: {RESUME_FROM_BACKUP}")
            target_brand_data = crawler.load_json(RESUME_FROM_BACKUP)[0]
            print(f"üìä ƒê√£ c√≥ {len(target_brand_data.get('car_types', []))} car types trong backup")
        else:
            print(f"\n‚ùå Kh√¥ng t√¨m th·∫•y file: {RESUME_FROM_BACKUP}")
            exit(1)
        
        # Get all car types from brand page
        print(f"\nüîç ƒêang l·∫•y danh s√°ch t·∫•t c·∫£ car types...")
        all_car_types = crawler.get_car_types(target_brand_data['href'])
        print(f"üìã T·ªïng s·ªë car types: {len(all_car_types)}")
        
        if START_CT > len(all_car_types):
            print(f"\n‚ùå Car type #{START_CT} kh√¥ng t·ªìn t·∫°i!")
            print(f"Ch·ªâ c√≥ {len(all_car_types)} car types")
            exit(1)
        
        # Crawl t·ª´ car type START_CT tr·ªü ƒëi
        for ct_idx in range(START_CT - 1, len(all_car_types)):
            car_type = all_car_types[ct_idx]
            actual_ct_idx = ct_idx + 1  # Index th·ª±c (1-based)
            
            print(f"\n{'='*60}")
            print(f"üöó [{actual_ct_idx}/{len(all_car_types)}] Car Type: {car_type['car_type']}")
            print(f"{'='*60}")
            
            try:
                # Get models
                models = crawler.get_models(car_type['href'])
                
                # ‚≠ê X·ª¨ L√ù 2 TR∆Ø·ªúNG H·ª¢P
                if actual_ct_idx == START_CT:
                    # ƒêang ·ªü gi·ªØa car type ‚Üí T√¨m car_type_data trong backup
                    car_type_data = None
                    for ct in target_brand_data['car_types']:
                        if ct['car_type'] == car_type['car_type']:
                            car_type_data = ct
                            break
                    
                    if not car_type_data:
                        # Ch∆∞a c√≥ trong backup ‚Üí t·∫°o m·ªõi
                        car_type_data = {
                            "car_type": car_type['car_type'],
                            "href": car_type['href'],
                            "models": []
                        }
                        target_brand_data['car_types'].append(car_type_data)
                    
                    # B·∫Øt ƒë·∫ßu t·ª´ model ti·∫øp theo
                    start_model_idx = START_MODEL  # ƒê√£ crawl xong model n√†y r·ªìi
                    print(f"‚è© B·ªè qua {start_model_idx} models ƒë√£ crawl")
                else:
                    # Car type m·ªõi ho√†n to√†n
                    car_type_data = {
                        "car_type": car_type['car_type'],
                        "href": car_type['href'],
                        "models": []
                    }
                    target_brand_data['car_types'].append(car_type_data)
                    start_model_idx = 0  # Crawl t·ª´ ƒë·∫ßu
                
                if not models:
                    print(f"   ‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y models")
                    continue
                
                print(f"  üìù T√¨m th·∫•y {len(models)} models")
                
                # ‚≠ê Crawl t·ª´ model start_model_idx tr·ªü ƒëi
                for model_idx in range(start_model_idx, len(models)):
                    model = models[model_idx]
                    actual_model_idx = model_idx + 1
                    
                    print(f"\n  üîß [{actual_model_idx}/{len(models)}] Model: {model['name']}")
                    
                    try:
                        # Get T·∫§T C·∫¢ categories v√† titles
                        categories = crawler.get_categories_and_titles(model['url'])
                        
                        model_data = {
                            "name": model['name'],
                            "url": model['url'],
                            "categories": categories
                        }
                        
                        car_type_data['models'].append(model_data)
                        
                        # Th·ªëng k√™
                        total_titles = sum(len(cat['titles']) for cat in categories)
                        print(f"    ‚úÖ {len(categories)} categories, {total_titles} titles")
                        
                        # ‚≠ê L∆ØU BACKUP SAU M·ªñI MODEL
                        backup_filename = f"{TARGET_BRAND}_CT{actual_ct_idx}_Model{actual_model_idx}.json"
                        crawler.save_to_json([target_brand_data], backup_filename)
                        
                    except Exception as e:
                        print(f"    ‚ùå L·ªói crawl model {model['name']}: {e}")
                        continue
                
                # ‚≠ê Reset start_model_idx cho car type ti·∫øp theo
                start_model_idx = 0
                
            except Exception as e:
                print(f"   ‚ùå L·ªói crawl car type: {e}")
                continue
        
        # Save final result
        crawler.save_to_json([target_brand_data], f"{TARGET_BRAND}_Complete.json")
        
        # Th·ªëng k√™ t·ªïng k·∫øt
        total_car_types = len(target_brand_data['car_types'])
        total_models = sum(len(ct['models']) for ct in target_brand_data['car_types'])
        total_categories = sum(
            len(model['categories']) 
            for ct in target_brand_data['car_types'] 
            for model in ct['models']
        )
        total_titles = sum(
            len(cat['titles'])
            for ct in target_brand_data['car_types']
            for model in ct['models']
            for cat in model['categories']
        )
        
        
        print(f"{'='*80}")
        print(f"   - Car Types: {total_car_types}")
        print(f"   - Models: {total_models}")
        print(f"   - Categories: {total_categories}")
        print(f"   - Titles: {total_titles}")
        
    except Exception as e:
        print(f"\n‚ùå L·ªói: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        print("\nüîí ƒê√≥ng browser...")
        crawler.close()
    
    print("\n‚ú® HO√ÄN TH√ÄNH!")
    
    